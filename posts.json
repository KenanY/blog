{"nav":[{"name":"videos","title":"Media","lead":"Stuff I've recorded/been recording talking about/been interviewed for","creator":"/user/maxogden","collaborators":[],"created_at":"2011-07-17T22:10:22.955Z","updated_at":"2012-04-13T03:18:54.612Z","published_version":"/version/a129cdacd09a1384ad040e55c1a178c8/1","published_on":"2012-04-13T03:18:54.612Z","views":10,"subjects":[],"entities":[],"children":["/section/63893d3f189d33ed413df199d5c369ca","/section/79838f37eb468f4740da8bba6251b475","/section/12e3c1e8b764343dcfc0d40a5711a645"],"type":["/type/document","/type/article"],"_id":"/document/maxogden/a129cdacd09a1384ad040e55c1a178c8","_rev":"201-c660108b928defe7e5d1128c1e8b2180","html":"<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n  <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n  <meta http-equiv=\"Content-Style-Type\" content=\"text/css\" />\n  <meta name=\"generator\" content=\"pandoc\" />\n  <meta name=\"author\" content=\"Max Ogden\" />\n  <title>Media</title>\n</head>\n<body>\n<div id=\"header\">\n<h1 class=\"title\">Media</h1>\n<h2 class=\"author\">Max Ogden</h2>\n</div>\n<p>Stuff I've recorded/been recording talking about/been interviewed for</p>\n<h1 id=\"screencasts\">Screencasts</h1>\n<p><a href=\"http://vimeo.com/18808177\">Get friendly with CouchDB</a></p>\n<p><a href=\"http://vimeo.com/20773112\">Hosting websites out of CouchDB</a></p>\n<p><a href=\"http://vimeo.com/26147136\">Interactive HTML5 CouchApps using node.couchapp.js</a></p>\n<p><a href=\"http://vimeo.com/17462239\">Awesomely simple web scraping with ScraperWiki</a></p>\n<p><a href=\"http://vimeo.com/19465653\">Five minute ScraperWiki intro</a></p>\n<p><a href=\"http://vimeo.com/18351837\">Cleaning up open data with Google Refine</a></p>\n<h1 id=\"speaking-engagements\">Speaking Engagements</h1>\n<p><a href=\"http://civic.mit.edu/blog/andrew/video-civic-media-session-bustling-with-information-cities-code-and-civics\">Panel at the MIT Media Lab: &quot;Bustling with Information: Cities, Code and Civics&quot;</a></p>\n<p><a href=\"http://vimeo.com/19850811\">CyborgCamp2010 keynote - Future webs and civic hacking</a></p>\n<p><a href=\"http://www.youtube.com/watch?v=2qbhIApkpsU\">CyborgCamp2010 Interview - The federated social web</a></p>\n<p><a href=\"http://www.youtube.com/watch?v=eyGM8jBcTRk\">NodeConf2011 Interview - Open data tools</a></p>\n<p><a href=\"http://www.youtube.com/watch?v=LzlxJ9LgXIc\">Government Open Source Conference - Why middleware is the key to a successful gov 2.0</a></p>\n<p><a href=\"http://vimeo.com/22869395\">Beyond Local Search - Panel on &quot;local data&quot;</a></p>\n<p>Portland, OR Civic Apps Mayoral Awards <a href=\"http://www.youtube.com/watch?v=T4AYi3QuWJU&amp;feature=player_embedded#t=0m51s\">Pt. 1 (Summer '10)</a> and the followup <a href=\"http://www.youtube.com/watch?v=2-ngRADwxuE&amp;feature=related\">Pt. 2 (Winter '10)</a></p>\n<p><a href=\"http://www.youtube.com/watch?v=VJ144mhPW4g&amp;t=16m45s\">Code for Oakland Apps for Communities Awards Presentations (on Open211.org)</a></p>\n<p><a href=\"http://thechangelog.com/post/8102338710/episode-0-6-5-code-for-america-with-erik-michaels-ober-a\">Changelog: The Github Podcast Code for America Interview with Erik and Max</a></p>\n<p><a href=\"http://vimeo.com/19688176\">Ignite Boston: Education, Technology and Libraries in Boston</a></p>\n<h1 id=\"articles\">Articles</h1>\n<p><a href=\"http://www.wweek.com/portland/blog-272-cyborg_camp_the_civic_web_will_help_you_find_cats_.html\">The Civic Web Will Help You Find Cats, Among Other Things</a></p>\n<p><em>Ogden talked about Code For America, an initiative that “enlists the brightest minds of the web industry into public service to use their skills to solve core problems facing our communities.” He talked about Paul Otlet, the forgotten father of information science, who may have pioneered the idea of the “like” button and the concept of recommendation systems. He talked in the context of libraries – imagine Netflix for public libraries, or an anthropomorphized library pushing book recommendations to you based on geolocation. How can we capitalize on the curiosity gap and encourage kids to seek information and collaborate? The spirit of open government is not about accessibility to data, but rather collaboration, and that is the engine behind Code for America</em>. - Georgy Cohen</p>\n</body>\n</html>\n"},{"name":"contact","title":"Contact","creator":"/user/maxogden","collaborators":[],"created_at":"2012-04-13T19:18:28.183Z","updated_at":"2012-04-13T23:15:48.144Z","published_version":"/version/62c8070223165ef657c00f55da6effb1/1","published_on":"2012-04-13T23:15:48.144Z","views":1,"subjects":[],"entities":[],"children":["/text/96a1a4508f6bcc718559dc128b2528e4"],"type":["/type/document","/type/article"],"_id":"/document/maxogden/62c8070223165ef657c00f55da6effb1","_rev":"5-3d539084b371546dc87bf8411b6ff22d","html":"<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n  <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n  <meta http-equiv=\"Content-Style-Type\" content=\"text/css\" />\n  <meta name=\"generator\" content=\"pandoc\" />\n  <meta name=\"author\" content=\"Max Ogden\" />\n  <title>Contact</title>\n</head>\n<body>\n<div id=\"header\">\n<h1 class=\"title\">Contact</h1>\n<h2 class=\"author\">Max Ogden</h2>\n</div>\n<p>max at maxogden dot com</p>\n<p>@maxogden</p>\n<p>github.com/maxogden</p>\n</body>\n</html>\n"},{"name":"projects","title":"Projects","lead":"A few recent projects (2011)","creator":"/user/maxogden","collaborators":[],"created_at":"2011-07-17T03:43:44.472Z","updated_at":"2011-12-31T03:55:04.532Z","published_version":"/version/b16b0d75651a613c5b04f778d573cf04/1","views":6,"subscribers":1,"subjects":[],"entities":[],"children":["/text/0000f1575801893360a6eab2a74d5ec1","/section/68e24e0b0e4b64c88796eb916331b4c1","/section/f4d23bd229dc5463f634fbe41f360152","/section/ab7b19be8728297540f38ee229533619","/section/d1d50bf0678850ec1c4203bffecd2029"],"type":["/type/document","/type/article"],"_id":"/document/maxogden/b16b0d75651a613c5b04f778d573cf04","_rev":"205-4761077ddb226ba0880e4558d4f853e7","published_on":"2011-07-17T04:45:21.603Z","html":"<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n  <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n  <meta http-equiv=\"Content-Style-Type\" content=\"text/css\" />\n  <meta name=\"generator\" content=\"pandoc\" />\n  <meta name=\"author\" content=\"Max Ogden\" />\n  <title>Projects</title>\n</head>\n<body>\n<div id=\"header\">\n<h1 class=\"title\">Projects</h1>\n<h2 class=\"author\">Max Ogden</h2>\n</div>\n<p>A few recent projects (2011)</p>\n<p>See the rest <a href=\"https://github.com/maxogden\">on github</a></p>\n<h1 id=\"open211.org\">Open211.org</h1>\n<p>A wiki-like crowdsourcing platform for social services and social programs targeted at dense urban communities.</p>\n<p>Link: <a href=\"http://open211.org\">http://open211.org</a></p>\n<div class=\"figure\">\n<img src=\"http://substance-assets.s3.amazonaws.com/32/3ab465b20820e98f308ef757af54d5/open211.png\" alt=\"Open211.org\" /><p class=\"caption\">Open211.org</p>\n</div>\n<p>Check out <a href=\"http://open211.org\">Open211.org</a> and add social programs for your city!</p>\n<h1 id=\"recline\">Recline</h1>\n<p>A partial port of Google Refine to Javascript/HTML5 as a CouchApp. Enables bulk data editing, importing and git-like distributed collaboration on datasets</p>\n<p>Link: <a href=\"http://max.ic.ht/sandbox/_design/recline/_rewrite\">http://max.ic.ht/sandbox/_design/recline/_rewrite</a></p>\n<div class=\"figure\">\n<img src=\"http://substance-assets.s3.amazonaws.com/b9/fbf09455739fb38e436e47da7c8428/removalist.png\" alt=\"Removalist UI\" /><p class=\"caption\">Removalist UI</p>\n</div>\n<h1 id=\"monocles\">Monocles</h1>\n<p>A highly exploratory and experimental port of Diaspora to Javascript/HTML as a CouchApp. Allows for private peer to peer social networking through federated social web protocols.</p>\n<p>Link: <a href=\"http://monocl.es\">http://monocl.es</a></p>\n<div class=\"figure\">\n<img src=\"http://substance-assets.s3.amazonaws.com/c9/d8ee90034331d8f476cbf12d214789/Screen-shot-2011-07-16-at-11.16.01-PM.png\" alt=\"« Enter Caption »\" /><p class=\"caption\">« Enter Caption »</p>\n</div>\n<h2 id=\"aed-mapper.com\">AED Mapper.com</h2>\n<p>A web UI for collecting locations of defibrillators so that they can be utilized by 911 response teams in emergency scenarios.</p>\n<p>Link: <a href=\"http://aedmapper.com\">http://aedmapper.com</a></p>\n<div class=\"figure\">\n<img src=\"http://substance-assets.s3.amazonaws.com/21/c4094e4b4446672bd3e18657da1366/Screen-shot-2011-07-17-at-2.16.41-PM.png\" /><p class=\"caption\"></p>\n</div>\n<h2 id=\"couchdb-archive-relaxed.tv\">CouchDB Archive + Relaxed.tv</h2>\n<p>Two open source community oriented projects relating to the CouchDB project:</p>\n<div class=\"figure\">\n<img src=\"http://substance-assets.s3.amazonaws.com/e5/6051cad547934bdfc402a523f07f8e/Screen-Shot-2011-08-03-at-4.13.39-PM.png\" alt=\"relaxed.tv\" /><p class=\"caption\">relaxed.tv</p>\n</div>\n<p>Relaxed.tv (<a href=\"http://relaxed.tv\">http://relaxed.tv</a>) is a video learning portal that crowdsources and aggregates tutorials and screencasts about CouchDB</p>\n<div class=\"figure\">\n<img src=\"http://substance-assets.s3.amazonaws.com/21/64a23bb2b0f6fcc18333dfe7600213/Screen-Shot-2011-08-03-at-4.14.27-PM.png\" alt=\"archive.couchdb.org\" /><p class=\"caption\">archive.couchdb.org</p>\n</div>\n<p>The Couch Archive (<a href=\"http://archive.couchdb.org\">http://archive.couchdb.org</a>) indexed mailing list and IRC conversations and makes them easily searchable for developers trying to learn or troubleshoot.</p>\n</body>\n</html>\n"}],"posts":[{"name":"replicating-large-datasets-into-html5","title":"Replicating large datasets into HTML5","lead":"Typed arrays, streaming json and IndexedDB!","creator":"/user/maxogden","collaborators":[],"created_at":"2012-04-14T01:00:46.567Z","updated_at":"2012-04-14T06:32:16.787Z","published_version":"/version/ab3376c449dd0941eaeaec4020286974/2","published_on":"2012-04-14T06:32:16.787Z","views":7,"subscribers":1,"subjects":[],"entities":[],"children":["/text/05d8ce8012daded648cbdfc78235f43f","/image/ae2b30974c98e77b8aabdb409370bdb6","/section/a3297ab4f034ec9b6a076099b55f062d","/section/25392645ec6044dd6f4297f0c3db2e45"],"type":["/type/document","/type/article"],"_id":"/document/maxogden/ab3376c449dd0941eaeaec4020286974","_rev":"49-e5bebebae813fcafe1ad7a656886440f","html":"<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n  <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n  <meta http-equiv=\"Content-Style-Type\" content=\"text/css\" />\n  <meta name=\"generator\" content=\"pandoc\" />\n  <meta name=\"author\" content=\"Max Ogden\" />\n  <title>Replicating large datasets into HTML5</title>\n</head>\n<body>\n<div id=\"header\">\n<h1 class=\"title\">Replicating large datasets into HTML5</h1>\n<h2 class=\"author\">Max Ogden</h2>\n</div>\n<p>Typed arrays, streaming json and IndexedDB!</p>\n<p>The time for fat clients is now! There are some key projects that were all released over the last month or two that allow for some really exciting (relatively) large data manipulation and storage in the browser. If you Voltron them together you can open up an AJAX request to arbitrarily large JSON response that you can stream in realtime into a persistent client-side data store. This workflow opens up a lot of new web application possibilities.</p>\n<div class=\"figure\">\n<img src=\"http://substance-assets.s3.amazonaws.com/a3/0edf9526c936ff041165dccbdf8927/687474703a2f2f692e696d6775722e636f6d2f4a304850652e706e67.png\" /><p class=\"caption\"></p>\n</div>\n<h1 id=\"pouch-crossfilter-and-jsonstream\">Pouch, Crossfilter and JSONStream</h1>\n<p><a href=\"https://github.com/mikeal/pouchdb\">PouchDB</a> is maintained by @daleharvey, @tilgovi, @mikeal and me. Dale <a href=\"http://arandomurl.com/2012/03/27/pouchdb-is-couchdb-in-the-browser.html\">wrote up</a> a post describing the project background but essentially Pouch gives you a CouchDB style API that stores data directly into your browser and uses IndexedDB which is available in <a href=\"http://caniuse.com/#search=indexeddb\">a few browsers</a> now. It even replicates with remote CouchDBs.</p>\n<pre><code>db.replicate.from(&#39;http://max.iriscouch.com/senators&#39;)\ndb.changes({ \n  onChange: function (change) {\n    console.log(change.doc)\n  })</code></pre>\n<p><a href=\"https://github.com/square/crossfilter\">Crossfilter</a> is by @mbostock. It uses HTML5 typed arrays (available in <a href=\"http://caniuse.com/#search=typed%20arrays\">a bunch</a> of browsers) to build fast in-memory indexes, sort of like Redis for your browser. Databases like Couch and Mongo let you write map/reduce to query your data, and Crossfilter provides the same functional query API to construct 'dimensions' (synonymous with facets, columns, maps, groups, indexes, etc) and then lets you do fast range queries against your dimensions. Make sure to check out <a href=\"http://square.github.com/crossfilter/\">the demo</a></p>\n<pre><code>var data = crossfilter([\n  {id: 5325, value: 1455}, {id: 2333, value: 6754}, {id: 8183, value: 3003} \n])\n\nvar byValue = data.dimension(function(d) { return d.value })    \nbyValue.filterRange([1000, 2000])\n\nconsole.log(byValue.top(10))\n// =&gt; {id: 5325, value: 1455}</code></pre>\n<p><a href=\"https://github.com/dominictarr/JSONStream\">JSONStream</a> is by @dominictarr and <a href=\"https://github.com/creationix/jsonparse\">jsonparse</a> is by @creationix. These libraries were originally written for Node point javascript but I forked em and got them to run on the client using the new HTML5 typed arrays. It turns out that Node uses Int32Array to implement it's Buffer object so I quickly <a href=\"https://github.com/maxogden/JSONStream/commit/ce43382f0bbae03ae7fb3752d20b32ab60cc5b2b\">patched JSONStream</a> to work if it gets included in the browser.</p>\n<p>To use node modules (of which there are about 9000 at the time of this post but not all will work using browserify) you can use the excellent <a href=\"https://github.com/substack/node-browserify\">node-browserify</a> by @substack. An advantage of this approach is code re-use between server and client as well as utilization of the excellent <a href=\"http://nodejs.org/api/\">node point javascript documentation</a> and avoidance of reinventing the wheel when it comes to the thousands of packages on NPM.</p>\n<pre><code>// from a command line\nnpm install browserify -g\nbrowserify -r events -r buffer -r stream -r util -r JSONStream -o browserify-bundle.js\n\n// then in your page\n&lt;script src=&quot;browserify-bundle.js&quot;&gt;&lt;/script&gt;\n&lt;script type=&quot;text/javascript&quot;&gt;\n  var stream = require(&#39;stream&#39;)\n  var util = require(&#39;util&#39;)\n  var JSONStream = require(&#39;JSONStream&#39;)\n  // now you can use them as if you were writing node!\n&lt;/script&gt;</code></pre>\n<p>JSONStream and jsonparse use Node's <a href=\"http://nodejs.org/api/stream.html\">Stream API</a>, which essentially gives you UNIX style pipes in JavaScript. I wrapped XMLHttpRequest2 in a simple object called XHRStream which gives you streaming access to data from AJAX requests.</p>\n<pre><code>  var xhr = new XMLHttpRequest()\n  xhr.open(&quot;GET&quot;, &quot;http://max.iriscouch.com/oakland_assessor/_all_docs?include_docs=true&quot;, true)\n  var stream = new XHRStream(xhr) \n  var json = JSONStream.parse([&#39;rows&#39;, /./, &#39;doc&#39;])\n  stream.pipe(json)\n  json.on(&#39;data&#39;, function(doc) {\n    // this will get called for each JSON object (available here as &#39;doc&#39;) that JSONStream parses\n  })</code></pre>\n<p>It's important that everything remain evented + streaming as you always want to start displaying data immediately in your application and not force your users to wait seconds or minutes for their data to download. Another approach would be to use WebSockets to get a data stream from the server but that requires servers that speak WebSockets whereas the benefit of JSONStream is that it can be made to work with any plain-Jane or fancy JSON API.</p>\n<p>One technical note is that XHR2 doesn't technically stream data, it buffers the entire response body into one giant string, available as xhr.responseText. Every time it gets a new chunk of data from the server it emits a readystatechange of 3 (so a request will usually emit something like 2 3 3 3 3 3 3 3 3 3 3 3 4) but instead of giving you access to a new data chunk it just appends the new chunk to the end of xhr.responseText and you have to manually keep track of each chunk's offset and slice it up yourself. This is an unfortunate API design which means that if you are downloading 10GB of JSON in a single XHR request then your browser will try to slowly fill up a single (and therefore not garbage collectable) 10GB string until you run out of memory.</p>\n<div class=\"figure\">\n<img src=\"http://substance-assets.s3.amazonaws.com/40/74dd07cb415f985bd54441b3452279/Screen-Shot-2012-04-13-at-5.44.11-PM.png\" alt=\"The first few properties being visualized\" /><p class=\"caption\">The first few properties being visualized</p>\n</div>\n<p>I wrote <a href=\"http://max.iriscouch.com/oakland_assessor/_design/streaming-xhr/_rewrite\">a quick demo</a> (code <a href=\"https://github.com/maxogden/streaming-xhr-example\">available here</a>) that streams an 80,000 row JSON query of assessed tax property values in Oakland into your browser and renders each property as a circle (radius is based on value) using d3.js. Because the data isn't being amortized or reduced in any way it quickly overwhelms browsers by creating an excess of expensive DOM nodes. I'm currently investigating streaming or &quot;on-line&quot; single pass statistics libraries written in JavaScript. @jasondavies tells me that his <a href=\"https://github.com/jasondavies/science.js\">science.js</a> library will soon have a single pass K-Means clustering algorithm.</p>\n<p>Again, it would be key here to have a clustering algorithm that could immediately start giving you cluster data to display as opposed to having to wait for all 80,000 documents to finish replicating to the client.</p>\n<h1 id=\"bringing-them-all-together\">Bringing them all together</h1>\n<p>Crossfilter and Pouch complement each other very, very well. The server-side Erlang CouchDB has a persistence layer and a query layer, whereas on the client these two features are achieved using Pouch and Crossfilter respectively. At this time Pouch isn't yet using a streaming JSON parser for it's replication implementation. It may be added in the future as a performance optimization. If your JSON is coming from CouchDB and you want it to get stored offline and sync it back later then use Pouch replication, otherwise consider playing with JSONStream as option for streaming large datasets into the client from traditional JSON APIs.</p>\n</body>\n</html>\n"},{"name":"open211","title":"Open211","lead":"An open source directory of social programs and services","creator":"/user/maxogden","collaborators":[],"created_at":"2011-07-30T23:16:01.713Z","updated_at":"2012-04-13T04:03:10.186Z","published_version":"/version/952de461faa96e24843f082f39abb33d/3","published_on":"2012-01-30T01:57:20.310Z","views":55,"subjects":["/attribute/c94d170416f21a768f17a61d8a93b561"],"entities":[],"children":["/image/c53814579efb99dabe7777d8120f791c","/text/19fb21de6d85159866469165e0a5a675","/image/ccb9e0c269fc716f1f21278376ca03f9","/text/8a4d22d07ddf219c280e80347b98e1da","/text/7851bad13bdc324511e4de3bff00a1c8","/text/e0e7c65fd88674a896ddf16d81445998","/text/6a19ca711e5204c2b17c94f843056f9b"],"type":["/type/document","/type/article"],"_id":"/document/maxogden/952de461faa96e24843f082f39abb33d","_rev":"123-66b45c38002eab3b039e88cdb1bd9457","html":"<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n  <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n  <meta http-equiv=\"Content-Style-Type\" content=\"text/css\" />\n  <meta name=\"generator\" content=\"pandoc\" />\n  <meta name=\"author\" content=\"Max Ogden\" />\n  <title>Open211</title>\n</head>\n<body>\n<div id=\"header\">\n<h1 class=\"title\">Open211</h1>\n<h2 class=\"author\">Max Ogden</h2>\n</div>\n<p>An open source directory of social programs and services</p>\n<div class=\"figure\">\n<img src=\"http://substance-assets.s3.amazonaws.com/f3/09faa1facb00dc2df501309abab535/open211-1.png\" alt=\"Open211.org\" /><p class=\"caption\">Open211.org</p>\n</div>\n<p>Since late May 2011 one of the side projects during my fellowship at <a href=\"http://codeforamerica.org\">Code for America</a> has been developing <a href=\"http://open211.org\">Open211.org</a> with <a href=\"http://twitter.com/#!/tilgovi\">Randall Leeds</a>, <a href=\"http://twitter.com/#!/briantrice\">Brian Rice</a> and a few other contributors. Open211 is a set of software tools for collecting, managing and distributing social service information. My inspiration for working on the project comes from one of the great local resources from my hometown, Portland, OR: <a href=\"http://www.streetroots.org/\">Street Roots</a>, who create jobs and media to support and educate the homeless population. They also distribute a ~100 page pocket directory of social programs biannually called the <a href=\"http://www.rosecityresource.org/\">Rose City Resource.</a></p>\n<div class=\"figure\">\n<img src=\"http://substance-assets.s3.amazonaws.com/cb/c19aab5acbccf29e553377bfd39db5/rrr.png\" alt=\"A Rose City Resource printed guide\" /><p class=\"caption\">A Rose City Resource printed guide</p>\n</div>\n<p>In a nutshell the Open211 project is attempting to introduce some of Tim O'Reilly's concepts of <a href=\"http://ofps.oreilly.com/titles/9780596804350/\"><em>Government as a Platform</em></a> to the traditionally closed data, proprietary software vendor dominated 211 space. For instance in Boston the transit authority (MBTA) released real time data feeds for busses and trains in a way that was only consumable by software developers. The interesting part of this approach is that they didn't actually hire a contractor to write the user facing website or app, but instead let the community make applications. With some community outreach and PR surrounding the bus data release they ended up with over 200 different applications to access bus and train arrival times. I think that a similar thing could happen with social programs on a national scale.</p>\n<p>The main purpose of the project is to collect accurate directory level information (hours, location, services offered, phone etc) and then make data centrally available to three audiences:</p>\n<p></p>\n<ul>\n<li><p><strong>Software developers</strong> - so that they can make software for accessing the data in new and interesting ways</p></li>\n<li><p><strong>Social program employees</strong> - to accurately recommend or 'redirect' patrons to other programs</p></li>\n<li><p><strong>Social program recipients</strong> - accessed through text message, by receiving Rose City Resource style printed guides or through a variety of other means that will get created by developers</p></li>\n</ul>\n<p>There are a lot of ways that this project could try to be more 'human', e.g. by providing feedback on services like Yelp, taking people in search of vocational training and pairing them with mentors, trying to track recidivism by allowing users to create profiles etc. but all of these things are mostly human problems and not technology problems and I'd rather not get too ambitious and complicated.</p>\n<p>All of my work is at this point free and open source and I'm happily getting compensated for this kind of work until at least the end of the year through the Code for America fellowship program. A highlight so far was being invited to the White House in Washington, D.C. to get honored by US Federal CTO Aneesh Chopra as an open data Champion of Change. Randall went and did the ole' dog and pony show and they made this <a href=\"http://www.govtech.com/e-government/White-House-Open-Data-Developers-Trade-Ideas.html#leeds\">great video from the event.</a></p>\n<p>We are currently seeking funding for this initiative so that we can hire part time data collection/software trainers/community evangelists in a few big cities to get Open211 deployed. At this point money for software development wouldn't help as I'm already taking on a full load of projects and have mostly built out the tools needed, now the limiting factor is real world usage so I'd like to get some people on the ground training programs how to use the tools and helping to collect directory information.</p>\n</body>\n</html>\n"},{"name":"diy-open-data-catalog","title":"DIY Open Data","lead":"How to start a public data catalog in your city","creator":"/user/maxogden","collaborators":[],"created_at":"2011-07-16T23:26:51.367Z","updated_at":"2011-11-09T22:29:06.232Z","published_version":"/version/aeb9a117d6a14d63b3ec81602fd8e5fa/2","views":69,"subscribers":3,"subjects":["/attribute/c94d170416f21a768f17a61d8a93b561"],"entities":[],"children":["/text/495495314e394e99ad2d5fdabf27520b","/image/df16eb6369014073ab03c841f1d6a1c4","/section/1dd7ab37fd2b4b2730b3417812578dab"],"type":["/type/document","/type/article"],"_id":"/document/maxogden/aeb9a117d6a14d63b3ec81602fd8e5fa","_rev":"176-0cb6f900a6577f8371f2b45fc4c6af92","published_on":"2011-12-02T20:37:32.004Z","html":"<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n  <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n  <meta http-equiv=\"Content-Style-Type\" content=\"text/css\" />\n  <meta name=\"generator\" content=\"pandoc\" />\n  <meta name=\"author\" content=\"Max Ogden\" />\n  <title>DIY Open Data</title>\n</head>\n<body>\n<div id=\"header\">\n<h1 class=\"title\">DIY Open Data</h1>\n<h2 class=\"author\">Max Ogden</h2>\n</div>\n<p>How to start a public data catalog in your city</p>\n<p>Around a year ago I started a project called <a href=\"http://www.pdxapi.com\">PDX API</a>, the purpose of which is to make it <em>super easy</em> for web developers to access a bunch of public data that various Portland, Oregon, area governments had just <a href=\"http://www.wweek.com/portland/article-12331-app_dancing.html\">started releasing</a>. The entire API is deceptively simple: its only dependency is a single <a href=\"https://github.com/couchbase/geocouch\">GeoCouch</a> instance. I have been lucky enough to meet various open data hackers from around the world this past year and many spinoff projects have started in response to PDX API. Some examples include <a href=\"http://twitter.com/chewbranca\">Russell Branca</a>'s <a href=\"http://seaapi.com\">SEAAPI</a> in Seattle, <a href=\"http://twitter.com/mheadd\">Mark Headd</a> &amp; co's <a href=\"http://phlapi.com\">PHLAPI</a> in Philadelphia and my own generalized version, <a href=\"http://civicapi.com\">CivicAPI</a>.</p>\n<p></p>\n<p>There are also some pretty awesome applications that have been built on top of PDX API. <a href=\"http://twitter.com/elsewisemedia\">Matt Blair</a> has done an amazing job of making tools that let anyone consume or create public datasets such as <a href=\"http://publicartpdx.com/\">public art</a>, <a href=\"http://pdxtrees.org/\">heritage trees</a> and <a href=\"http://poetrybox.info/\">poetry boxes</a>.</p>\n<div class=\"figure\">\n<img src=\"http://substance-assets.s3.amazonaws.com/67/b8b0c816774ddb28e8c56a7b0706a3/Screen-shot-2011-07-17-at-12.17.08-AM.png\" alt=\"« Enter Caption »\" /><p class=\"caption\">« Enter Caption »</p>\n</div>\n<h1 id=\"starting-your-city-api\">Starting (your city) API</h1>\n<p>This blog post is a call for participation. As more data gets made available from different places more applications can be shared and adapted for use in those places. The excellent <a href=\"http://www.civiccommons.com/\">Civic Commons</a> project will ensure that plenty of applications will be available for you to adapt to your city's data, and everything listed so far in this blog post is open source and can also be used. All you need to do, dear reader, is follow these steps:</p>\n<ul>\n<li><p>Get raw data from your local government</p></li>\n<li><p>Transform the raw data into nice, happy data</p></li>\n<li><p>Put the data into GeoCouch so it can be shared</p></li>\n<li><p>Spread the word</p></li>\n</ul>\n<p></p>\n<h2 id=\"getting-the-data\">Getting the data</h2>\n<p>Many cities have participated in what are called <strong>open data initiatives</strong>, which means they take taxpayer dollar produced raw datasets and make them publicly accessible. These include things like library and hospital locations, real time bus GPS feeds and <a href=\"http://granicus.sandiego.gov/ViewPublisher.php?view_id=3\">real time streams</a> (via <a href=\"http://www.granicus.com/\">Granicus</a>) of your local city council chamber.</p>\n<p>PDX API works with most types of data, but you shouldn't reinvent the wheel. Make sure to check the <a href=\"http://wiki.civiccommons.org/\">CivicCommons wiki</a> and see if there is an existing web accessible API that makes your city's data available.</p>\n<p>Reach out to your local Mayor's office and try to figure out if there is any raw data available from your city. If you want some advice shoot me an email! At <a href=\"http://codeforamerica.org/\">Code for America</a> we have a great government relations director, <a href=\"http://twitter.com/alissa007\">Alissa Black</a>, who can help navigate the political aspects of this step.</p>\n<p>Recently I found an <a href=\"ftp://ftp.ci.austin.tx.us/GIS-Data/Regional/\">FTP server</a> hosted by the City of Austin, Texas containing over 80 gigabytes of public data. This makes Austin a perfect candidate: they have a bunch of data available but it is hard to download and work with if you aren't a GIS technician.</p>\n<h2 id=\"making-happy-data\">Making happy data</h2>\n<p>I've spent much of the last year looking for tools that help ease the process of taking raw data and turning it into something nice and accessible. Perhaps the most useful utility I've worked with is <a href=\"http://code.google.com/p/google-refine/\">Google Refine</a>, a &quot;power tool for working with messy data.&quot;</p>\n<p>To learn Refine, I'd recommend watching the screencasts available from David Huynh, the author of Refine, over on the <a href=\"http://code.google.com/p/google-refine/\">Google Refine page.</a> I also did a tutorial screencast that is <a href=\"http://vimeo.com/18351837\">available here</a>.</p>\n<h2 id=\"putting-your-data-online\">Putting your data online</h2>\n<p>Up until this point I've been assuming you've got access to some public data and server space available to host it. If not, let me know, and I can try to connect you with the right resources. The purpose of my <a href=\"http://civicapi.com/\">CivicAPI</a> project is to host anyone's public data, so if you don't want to go through the hassle of setting up our own domain, I would be happy to host the data for you.</p>\n<p>On your server you will need to be running a copy of <a href=\"http://www.couchbase.com/downloads/couchbase-server/community\">GeoCouch</a>. Check out my blog post describing how to <a href=\"http://maxogden.com/#/blog/installing-geocouch\">install GeoCouch and shp2geocouch</a>. The majority of open geographic data is released in the ESRI Shapefile format, so I wrote <a href=\"https://github.com/maxogden/shp2geocouch\">shp2geocouch</a>, a command line Ruby gem utility that processes Shapefiles and converts them into GeoCouch databases. A pretty good review/tutorial for using shp2geocouch is available from the aforementioned Mark Headd on the <a href=\"http://blog.tropo.com/2010/12/25/unlocking-government-data-with-tropo-and-open-source-software/\">Tropo developer blog</a>.</p>\n<p>Another useful utility that I've written is <a href=\"https://github.com/maxogden/refine-uploader\">an extension</a> for Refine that lets you export data directly to any compatible webserver (including CouchDB and GeoCouch). Once your data is in GeoCouch, you can use my <a href=\"https://github.com/maxogden/removalist\">Removalist</a> app to export your database back and forth forever between GeoCouch and Google Refine, making bulk edits and cleanups each time.</p>\n<p>The third tool in the open data trifecta is <a href=\"http://scraperwiki.com\">ScraperWiki</a>. ScraperWiki is a browser based editor that lets you write scrapers -- bits of code that go and fetch messy data. ScraperWiki then takes care of running your scrapers every day so that they'll go and fetch new content.</p>\n<p>ScraperWiki is useful in cases where raw data is not available directly from your city, but it instead is embedded inside of a page on the city's website. For instance, some of the other fellows at Code for America and I wrote a <a href=\"http://scraperwiki.com/search/boston/\">variety of scrapers</a> for the City of Boston's website, ranging from datasets like phone numbers for every department in city hall to the name of every street that you're allowed to park on during a snow storm. <a href=\"http://vimeo.com/17462239\">Here's a screencast</a> that I made to demonstrate ScraperWiki by scraping all of the restaurants in Chicago off of their health inspection website.</p>\n<p>When setting up your API in Couch there are a bunch of resources at your disposal. If you are unfamiliar with CouchDB, you might find my <a href=\"http://vimeo.com/18808177\">intro screencast</a> useful.</p>\n<p>There are also a lot of helpful links at <a href=\"http://couchapp.org/page/index\">CouchApp.org.</a> For getting data out of GeoCouch in a bunch of different ways (like KML or radius queries) there is the <a href=\"https://github.com/maxogden/geocouch-utils\">GeoCouch Utils</a> project.</p>\n<p>CouchDB is capable of hosting not only your data but also your application (as long as your application is written in JavaScript and HTML5). The most awesome part of Couch is that every single dataset is fully replicable to any other Couch. If there is a dataset of 20,000 bus stops that you want to download and play with, the easiest way is to install Couch on your local computer and make a clone of all 20,000 by copying them (also known as &quot;replicating&quot;) to your local Couch. This also works both ways, so if you do all of your data cleanup on your local Couch and you want to host the datasets on <a href=\"http://civicapi.com/\">CivicAPI</a>, let me know, and I will give you permission to replicate directly to the CivicAPI Couch. You can learn more about Couch's replication <a href=\"http://guide.couchdb.org/draft/replication.html\">here</a>.</p>\n<p>The other benefit of replication is that it makes for robust decentralized networks. If your server gets its <a href=\"http://www.readwriteweb.com/archives/datagov_7_other_sites_to_shut_down_after_budgets_c.php\">budget cut</a>, it's only a few clicks to replicate all of your datasets to another Couch powered API.</p>\n<h2 id=\"tell-the-world\">Tell the world</h2>\n<p>Once your data is online, get out there are tell people about it! Try hooking up some applications to the API, show it off at local user groups, announce it on Twitter (cc me for a retweet) and don't forget to add it to the <a href=\"http://wiki.civiccommons.org/\">CivicCommons wiki</a>.</p>\n</body>\n</html>\n"},{"name":"gut-hosted-open-data-filets","title":"Gut: Hosted Open Data Filet Knives","lead":"HTTP Unix pipes for Open Data","creator":"/user/maxogden","collaborators":[],"created_at":"2011-12-02T19:08:21.365Z","updated_at":"2011-12-02T21:02:59.564Z","published_version":"/version/759866b37e7a56dd0872e3c0bb8be6ae/1","views":14,"subscribers":1,"subscribed":true,"subjects":["/attribute/c94d170416f21a768f17a61d8a93b561"],"entities":[],"children":["/image/ec47125c8eaf9792f92be9eb2f505d9d","/section/b650159f04be17d9b69b0d6d527267f7","/section/833f4108c21a702e6cac13d9c910ff1e","/section/245792627e6e29dbec7f6fde99eb81bf","/section/2167af97f1d54aa3f11856b1703e49e3"],"type":["/type/document","/type/article"],"_id":"/document/maxogden/759866b37e7a56dd0872e3c0bb8be6ae","_rev":"210-7085be5929982fc86e21a191cfded0a3","published_on":"2011-12-02T20:36:31.424Z","html":"<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n  <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n  <meta http-equiv=\"Content-Style-Type\" content=\"text/css\" />\n  <meta name=\"generator\" content=\"pandoc\" />\n  <meta name=\"author\" content=\"Max Ogden\" />\n  <title>Gut: Hosted Open Data Filet Knives</title>\n</head>\n<body>\n<div id=\"header\">\n<h1 class=\"title\">Gut: Hosted Open Data Filet Knives</h1>\n<h2 class=\"author\">Max Ogden</h2>\n</div>\n<p>HTTP Unix pipes for Open Data</p>\n<h1 id=\"not-invented-here\">Not Invented Here</h1>\n<p>A topic that has fascinated me for years now is (broadly speaking) <a href=\"http://en.wikipedia.org/wiki/Nationalism\">nationalism</a>. In the world of the internet that essentially boils down to something like &quot;<em>I am a Python programmer and this project is written in Java! Ignored!&quot;</em>. It is this behavior (which I see a lot in programming, mostly manifested as &quot;<a href=\"http://en.wikipedia.org/wiki/Not_invented_here\">Not invented here</a>&quot;) that leads to a bunch of solutions to the same problem written in a bunch of different languages where many of the solutions are half-baked.</p>\n<p>For a concrete example consider open data catalogues. As evidenced by <a href=\"http://datacatalogs.org\">datacatalogs.org</a> there are a ton of different solutions to the same set of problems, namely hosting open data. Having a rich ecosystem is a good thing, but I believe that there is a common open data infrastructure layer that we aren't maximizing our collaborating on: the conversion of data between different formats.</p>\n<p>Wouldn't it be great if I, as a Javascript developer, could use the awesome data conversion libraries available in Java like <a href=\"http://poi.apache.org\">Apache POI</a>? Or if Ruby developers could use Python packages like <a href=\"https://github.com/onyxfish/csvkit\">csvkit</a> (which contains the super useful csvclean utility). The good news is that the internet has settled on a common language for crossing these language barriers: HTTP and JSON. Additionally, nowadays the web is filled with hosted services (see SaaS, PaaS). There are numerous platforms where hosted services can be deployed for free (Google App Engine, Heroku, Dotcloud, Nodejitsu, etc).</p>\n<h1 id=\"unix-pipes\">Unix pipes</h1>\n<p>On the Unix command line there are a bunch of useful single purpose utilities. The <a href=\"http://en.wikipedia.org/wiki/Unix_philosophy\">Unix philosophy</a> is <em>&quot;write programs that do one thing and do it well. Write programs to work together. Write programs to handle text streams, because that is a universal interface&quot;</em>. The Unix command <code>wc</code> is a great example. You give it a bunch of text and it will count the number of words, lines and characters. Combined with the <code>cat</code> command, which reads and file and dumps out all the text, you can use a Unix pipe (the | character) to 'pipe' the data that <code>cat</code> dumps out into <code>wc</code>:</p>\n<pre><code>cat awesomeText.txt | wc\n21      55     507</code></pre>\n<h1 id=\"what-is-gut\">What is gut</h1>\n<p>Taking heavy inspiration from unix pipes, HTTP and JSON I have come up with a modest proposal for how we might share our best tools for various data conversion jobs as hosted web services. I'm calling it gut, as in gutting a fish and getting the yummy filet out while leaving behind all of the junk.</p>\n<p>Here's a simple example of how a gut server would work that takes in a CSV file and returns JSON data. As a developer using the gut server to process my CSV file I would send the following HTTP request containing my CSV data:</p>\n<pre><code>POST / HTTP/1.1\nUser-Agent: curl\nHost: gutcsv.nodejitsu.com\nAccept: */*\nContent-Length: 64\nContent-Type: application/x-www-form-urlencoded\n\nname,appearance\nchewbacca,hairy\nbill,nonplussed\nbubbles,relaxed</code></pre>\n<p>This is what the gut server would give me back:</p>\n<pre><code>POST / HTTP/1.1\nhost: gutcsv.nodejitsu.com\ncontent-type: application/json\ncontent-length: 186\nConnection: close\n  \n{\n  &quot;headers&quot;: [\n    {\n      &quot;name&quot;: &quot;name&quot;\n    },\n    {\n      &quot;name&quot;: &quot;appearance&quot;\n    }\n  ],\n  &quot;rows&quot;: [\n    {\n      &quot;name&quot;: &quot;chewbacca&quot;,\n      &quot;appearance&quot;: &quot;hairy&quot;\n    },\n    {\n      &quot;name&quot;: &quot;bill&quot;,\n      &quot;appearance&quot;: &quot;nonplussed&quot;\n    },\n    {\n      &quot;name&quot;: &quot;bubbles&quot;,\n      &quot;appearance&quot;: &quot;relaxed&quot;\n    }\n  ]\n}</code></pre>\n<p>Essentially I am piping data from my computer through a gut server and when it comes back it is in the new format. In this example I used the node.js hosting platform Nodejitsu to deploy my CSV-to-JSON code so that it is available to anyone in the world who can make an HTTP request.</p>\n<h1 id=\"voltron-assemble\">Voltron, assemble!</h1>\n<p>If you are writing code that converts data from one format to another, consider also exposing your solution in the form of a gut server! Last year I had great success at <a href=\"http://opendataday.org\">International Open Data Day</a> teachin ScraperWiki because it scaled out well to a room full of people with different programming backgrounds. I think that writing these lightweight data converters/massagers/transformer servers is also a task that anyone can tackle in a short amount of time.</p>\n<p>There is a <a href=\"https://github.com/maxogden/gut\">Github project</a> that contains the current gut servers that I have been working on and also a <a href=\"https://github.com/maxogden/gut/wiki/List-of-gut-servers\">wiki page</a> where you can add your gut server to the list. Once there are a handful of gut servers we can start working on more extensive discovery and testing tools (ensuring gut server availability, better documentation, web based gut server API playground, etc).</p>\n</body>\n</html>\n"},{"name":"mobile-crowdsourcing-interfaces","title":"Mobile Crowdsourcing Interfaces","lead":"Some experiments in collecting images + location from mobile devices","creator":"/user/maxogden","collaborators":[],"created_at":"2011-07-29T21:01:50.042Z","updated_at":"2012-02-25T21:49:40.772Z","published_version":"/version/fda8ed24337445a9893a9471ecf4bdfc/1","published_on":"2011-07-30T17:20:49.949Z","views":3,"subjects":["/attribute/c94d170416f21a768f17a61d8a93b561"],"entities":[],"children":["/text/679b600b9d252a9677bdd0aa52a01208","/text/c25201d3fcc74ae95cd31d46eaa6b9cf","/image/a8e307e9cc18d0bea644d4b8746e2da7","/text/06514dbfbe1a713365c96bce42139b5d","/section/ca68c1c6f31317117344a8109885fb09","/section/49f2f17fd71b268bd64b7ba3fb6d6d14"],"type":["/type/document","/type/article"],"_id":"/document/maxogden/fda8ed24337445a9893a9471ecf4bdfc","_rev":"1289-88252aaf31bd8171893299109c613067","html":"<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n  <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n  <meta http-equiv=\"Content-Style-Type\" content=\"text/css\" />\n  <meta name=\"generator\" content=\"pandoc\" />\n  <meta name=\"author\" content=\"Max Ogden\" />\n  <title>Mobile Crowdsourcing Interfaces</title>\n</head>\n<body>\n<div id=\"header\">\n<h1 class=\"title\">Mobile Crowdsourcing Interfaces</h1>\n<h2 class=\"author\">Max Ogden</h2>\n</div>\n<p>Some experiments in collecting images + location from mobile devices</p>\n<p>Over the last couple of years I've been fascinated with technology that collects and aggregates mappable information. Specifically I have been searching for or creating tools that have these properties:</p>\n<p></p>\n<ul>\n<li><p>Lets users discover things near them, allows submission of location, photos and text</p></li>\n<li><p>Built on an open data API so that others can access and build on the data</p></li>\n<li><p>Easy for new developers to clone and deploy the app. A big roadblock on this front has been the Apple App Store. $100 and weeks of back and forth just to publish a public data app that uploads photos? Rubbish.</p></li>\n</ul>\n<p></p>\n<p>My first foray into &quot;civic web&quot; applications was in the form of a project called Portland Smells, which was a map-wiki that collected smelly places (both good and bad) and threw them on a map.</p>\n<div class=\"figure\">\n<img src=\"http://substance-assets.s3.amazonaws.com/19/65ca390f66b525383a3779dc12844c/psmells.png\" alt=\"The now defunct www.portlandsmells.com\" /><p class=\"caption\">The now defunct www.portlandsmells.com</p>\n</div>\n<p>Portland Smells was meant to be more of an art project than a public utility. That is, until a local government representative contacted me regarding a project that was being built out (now available at <a href=\"http://www.whatsinourair.org/\">whatsinourair.org</a>) to help report and monitor illegal chemical dump sites in the Portland area. Apparently the functionality created for Portland Smells was greatly needed for social and environmental justice. Needless to say I got hooked on the idea of open data.</p>\n<h1 id=\"from-smells-to-carts-and-cats\">From smells to carts and cats</h1>\n<p>After mapping smells I took a stab at an application to map <a href=\"http://www.foodcartsportland.com/\">food carts</a> that also allowed users to keep food cart locations, menus and hours of operation up to date. The app was made in early 2010 (<a href=\"https://github.com/maxogden/pdx-food-carts-mobile\">code here</a>) using the Titanium Mobile framework and used <a href=\"http://pdxapi.com\">PDX API</a> as the backend. Unfortunately Microsoft also decided to <a href=\"http://spinnerin.tumblr.com/post/1014484948/why-contribute-to-a-community-project-when-you-can\">release a closed source</a> food cart application (with a huge marketing budget) which effectively killed my excitement about the cart mapping project.</p>\n<div class=\"figure\">\n<img src=\"http://substance-assets.s3.amazonaws.com/7d/5ba7d7b536bf39861123d0d043e2ee/Screen-Shot-2011-07-29-at-5.23.43-PM.png\" alt=\"The now defunct CartsPDX\" /><p class=\"caption\">The now defunct CartsPDX</p>\n</div>\n<p>Developing a cross platform mobile application using <a href=\"http://www.appcelerator.com/products/titanium-mobile-application-development/\">Titanium</a> had it's fair share of speedbumps, including buggy tools, lack of features and weird javascript DSLs. I would imagine that now, a year and a half later, a lot of these issues have been addressed.</p>\n<p>Then I got <a href=\"http://www.wweek.com/portland/print-article-12542-print.html\">obsessed with cats</a>. While walking around neighborhoods in Portland you will run into them all over the freaking place. I started to wonder if certain cats were always out on porches and what neighborhoods had the highest cat density. Someone told me that if the humane society had access to cat density data then they could better target spaying and neutering education initiatives. I started <a href=\"http://www.vimeo.com/tag:catmapper\">recording videos</a> of cats that I found while walking around and prototyped a pure webapp CartsPDX-like interface using jQuery Mobile: <a href=\"http://catmapper.com\">catmapper.com</a></p>\n<div class=\"figure\">\n<img src=\"http://substance-assets.s3.amazonaws.com/6b/5483054d6c36b98f15e2c3e1cee5db/Screen-Shot-2011-07-29-at-5.53.32-PM.png\" alt=\"catmapper.com\" /><p class=\"caption\">catmapper.com</p>\n</div>\n<h1 id=\"collecting-photos\">Collecting photos</h1>\n<p>The biggest tradeoff to consider when building mobile web apps vs. native apps is the inability of most mobile web browsers to natively upload photos. Generally speaking native applications require more commitment from the application developer but provide a better user experience. Here I will share some different approaches to the problem of collecting photos and locations</p>\n<h2 id=\"wrapping-web-apps-in-native-functionality-with-phonegaptitanium\">Wrapping web apps in native functionality with Phonegap/Titanium</h2>\n<p>Rather than learning Objective-C and Java I would strongly prefer to stick to my high level dynamic language of choice, JavaScript. Both <a href=\"https://github.com/phonegap\">Phonegap</a> and <a href=\"https://github.com/appcelerator\">Titanium</a> provide a JavaScript DSL that abstracts native functionality for the developer and has the ability to 'compile' to multiple mobile platforms.</p>\n<p>Personally I am more a fan of Phonegap because of the team's commitment to support and foster HTML5 and their ultimate goal of making Phonegap itself obsolete through cross platform open web standards. This also results in more of a &quot;modern&quot; JavaScript feel than what Titanium offers.</p>\n<p>Regardless of which tools you choose to create native applications, the most tedious part will always be registering app store developer licenses, packaging and submitting applications for approval.</p>\n<p>The <a href=\"https://github.com/andrewgleave/OpenElm\">OpenElm</a> Project is a great open source Phonegap application built using jQuery Mobile for the UI and CouchDB on the server. Andrew Gleave, lead developer on the project, even went the extra mile and wrote an <a href=\"https://github.com/andrewgleave/CouchDBAttachmentUploader\">attachment uploader plugin</a> for iOS Phonegap devices that makes uploading files and photos super smooth.</p>\n<div class=\"figure\">\n<img src=\"http://substance-assets.s3.amazonaws.com/42/e7a15aa17cc6e8901be8d0cdba767f/Screen-Shot-2011-07-29-at-6.07.25-PM.png\" alt=\"OpenElm\" /><p class=\"caption\">OpenElm</p>\n</div>\n<h2 id=\"email-submissions\">Email Submissions</h2>\n<p>Many smartphones can interpret mailto: links and launch email composition forms pre-loaded with the recipient address from the mailto:. On Android devices users are given the option of adding attachments to an existing message, but on iOS there is no way to add an attachment to a message being composed - you have to start the interaction by visiting the photo browser and choosing 'Share', however there is no way to initialize the photo sharing interface from a mobile web app. This severely complicates the UX for iOS users.</p>\n<p>Regardless of the UX complications, email can be a nice and simple way to offer photo submissions without going through the rigamarole of registering native developer credentials. There are two ways that I've tried initiating email submissions</p>\n<h3 id=\"visiting-a-mobile-web-app-first-and-being-given-a-mailto-link\">Visiting a mobile web app first and being given a mailto: link</h3>\n<p>Catmapper.com (source), a prototype application, is an example of first collecting location and then providing an email address where photos can be submitted later. The requires that users remember to visit catmapper.com when they are walking down the street and visit a cat, and also that they are able to take an email address from a website and successfully email photos to it.</p>\n<div class=\"figure\">\n<img src=\"http://substance-assets.s3.amazonaws.com/cb/3ad60ae34b1ac58234e55c7f7806b8/Screen-Shot-2011-07-29-at-5.53.32-PM.png\" alt=\"Catmapper Prototype\" /><p class=\"caption\">Catmapper Prototype</p>\n</div>\n<h3 id=\"sending-photos-to-a-particular-email-address-first-and-receiving-instructions-in-a-reply-email\">Sending photos to a particular email address first and receiving instructions in a reply email</h3>\n<p>Pictured below is another now defunct prototype that was meant to demonstrate how defibrillator locations might be submitted by thoughtful citizens who encounter them. First the smartphone must have GPS enabled so that the photos contain coordinates in the embedded EXIF data. Photos are emailed in and users will receive a response email containing a visualization of the GPS coordinates from the EXIF information. Upon tapping on the map they are taken to a catmapper-style mobile interface that lets them update the specific coordinates for their submission.</p>\n<div class=\"figure\">\n<img src=\"http://substance-assets.s3.amazonaws.com/a8/b4d051dc6477185ac39f861f9539b2/Screen-Shot-2011-07-29-at-6.12.44-PM.png\" alt=\"AED Mapper prototype\" /><p class=\"caption\">AED Mapper prototype</p>\n</div>\n<p>This particular email processing workflow was implemented using a Ruby server that accessed a gmail account through IMAP. You can get the code by visiting the older commits in <a href=\"https://github.com/codeforamerica/aedmapper\">this repository</a>. Warning: trying to write a realtime IMAP client for Gmail was pretty painful and prompted me to write a realtime node.js email processor, <a href=\"http://github.com/maxogden/haraka-couchdb\">haraka-couchdb</a>.</p>\n<h2 id=\"tweets-with-geolocation-and-photos\">Tweets with geolocation and photos</h2>\n<p>There are tons of native Twitter applications that allow uploading to third party image sharing services like yfrog or twitpic. Some colleagues of mine at Code for America took a <a href=\"https://github.com/codeforamerica/muralmapper\">Node.js daemon</a> from civic hacker Mark Headd and extended it to save images + latitude/longitude from tweets into CouchDB. This approach requires users to enable geolocation in their Twitter client. It also has an interesting social property that can photo sharing engagement.</p>\n<div class=\"figure\">\n<img src=\"http://substance-assets.s3.amazonaws.com/6c/4c0b8bb4f10a849ecd72dfe8fd7100/Screen-Shot-2011-07-29-at-6.30.27-PM.png\" alt=\"Public Art Mapper\" /><p class=\"caption\">Public Art Mapper</p>\n</div>\n<h2 id=\"mms-gateways\">MMS Gateways</h2>\n<p>This would be super cool for bridging the gap from smartphones to feature phones but unfortunately is not supported by the major telephony API providers (Twilio and Tropo).</p>\n<h2 id=\"desktop-uploading\">Desktop uploading</h2>\n<p>Photos often don't need to be uploaded in the field and can be synced and uploaded at a later time. This broadens the range of cameras that can be used to capture images and lets users without smartphones participate in using your application. Here is another prototype, <a href=\"http://aedmapper.com/\">aedmapper.com</a> (<a href=\"https://github.com/codeforamerica/aedmapper\">code here</a>), that is meant to show how to collect accurate information about defibrillator locations using address autocompletion, web based mapping and drag and drop photo upload forms.</p>\n<div class=\"figure\">\n<img src=\"http://substance-assets.s3.amazonaws.com/43/af19e69b4d2d349432e055a134840f/Screen-Shot-2011-07-29-at-6.41.03-PM.png\" /><p class=\"caption\"></p>\n</div>\n<p>Using the same code I created two separate <a href=\"http://open211.org/#upload\">upload forms</a> for another project, Open211, a directory of social services. One form collects locations while the other offers dropbox-like attachment uploading functionality.</p>\n<div class=\"figure\">\n<img src=\"http://substance-assets.s3.amazonaws.com/c9/8bbadecdb367cd0c474c30231212ba/Screen-Shot-2011-07-30-at-1.14.07-PM.png\" alt=\"Open211.org\" /><p class=\"caption\">Open211.org</p>\n</div>\n<h2 id=\"pen-paper\">Pen + Paper</h2>\n<p>The most ubiquitous mobile crowdsourcing interface comes in the form of <a href=\"http://walking-papers.org/\">walking papers</a>, a project by Michal Migurski. Designed as a tool for collecting data for OpenStreetMap, the computer vision technology that powers walking papers could also prove useful for large scale low technology data collection deployments. I haven't yet had a chance to use this tech but am itching at the opportunity.</p>\n<div class=\"figure\">\n<img src=\"http://substance-assets.s3.amazonaws.com/3d/36cd4193c7bd70371e4ca41da172ab/5042873815_d1f17898fb_z.png\" alt=\"www.flickr.com/photos/harrywood/5042873815/\" /><p class=\"caption\">www.flickr.com/photos/harrywood/5042873815/</p>\n</div>\n<p>OpenStreetMap contributors have gathered for <em>mapping parties,</em> which consist of treks through unmapped neighborhoods where participants document what they see by writing down landmarks, roads, parks and playgrounds on a special map that can be scanned and uploaded to walking-papers.org where they are then georectified and traced directly onto OpenStreetMap.</p>\n</body>\n</html>\n"},{"name":"little-coders","title":"Little Coders","lead":"Elementary school programming","creator":"/user/maxogden","collaborators":[],"created_at":"2011-07-17T02:56:34.064Z","updated_at":"2011-07-30T17:57:25.208Z","published_version":"/version/aaebe058df3b88283c95bc63be396344/1","published_on":"2011-07-17T02:56:48.555Z","views":13,"subjects":["/attribute/c94d170416f21a768f17a61d8a93b561"],"entities":[],"children":["/text/2700d938013b7e592fa15dfafb3bd3a8","/image/8869958892adf04d7bd30203c8b71faa","/text/c0f5f108a4b3704d04c5e6abfc92f11e","/text/e53fc96b3d20668f41d9547af6e0efd4","/text/feed7d411aa2b47adb4d672008fbcc17","/text/d529de36b2475eebd90ed9efbfe02666","/text/5d5eb09d5e1ff0c53a141502a98c29fe","/text/2745c93ad412c9c1e1788d03121eb553"],"type":["/type/document","/type/article"],"_id":"/document/maxogden/aaebe058df3b88283c95bc63be396344","_rev":"52-08f2bab760b579531cf887a55da70fdb","html":"<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n  <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n  <meta http-equiv=\"Content-Style-Type\" content=\"text/css\" />\n  <meta name=\"generator\" content=\"pandoc\" />\n  <meta name=\"author\" content=\"Max Ogden\" />\n  <title>Little Coders</title>\n</head>\n<body>\n<div id=\"header\">\n<h1 class=\"title\">Little Coders</h1>\n<h2 class=\"author\">Max Ogden</h2>\n</div>\n<p>Elementary school programming</p>\n<p>My first foray into programming was in 1999 and happened accidentally inside the Starcraft custom level editor. I would sit, fully <a href=\"http://cyborganthropology.com/Paracosmic_Immersion\">immersed in a paracosm</a>, and create complex storylines that the player could play through, one small objective at a time. You could specify, for instance, that if a Zergling walks onto a particular platform then an event should trigger. You could subscribe other elements of the game to listen for events on that platform and perform other callbacks. I was getting exposed to event based publish/subscribe asynchronous programming without realizing it.</p>\n<div class=\"figure\">\n<img src=\"http://substance-assets.s3.amazonaws.com/f3/5b3f0ae4d5e9ee22ec932ea8a8c623/starcraft-triggers.png\" alt=\"the default global triggers inside any Starcraft scenario\" /><p class=\"caption\">the default global triggers inside any Starcraft scenario</p>\n</div>\n<p>When my older brother asked me how to get his nine year old interested in programming, I told him to look for a programming environment that sacrifices practicality for imagination. Nine year olds aren't worried about vendor lock in or job security, they are only going to get hooked if the feel like they are playing a video game (in my case with Starcraft I was literally playing a video game). A good example of this mentality is <a href=\"http://hacketyhack.heroku.com/\">Hackety Hack</a> by _why the lucky stiff. Hackety Hack aims to &quot;offer a place for plainspeople to tinker with code&quot;. Here is a wonderful video from the <a href=\"http://www.flong.com/projects/artandcode/\">Art &amp;&amp; Code Symposium</a> where _why talks about his thoughts on programming education for youngsters and shows off Hackety Hack. <a href=\"http://vimeo.com/5047563\">Highly recommended viewing.</a></p>\n<p>Near the end of the above video, _why describes a card game called Kaxxt that is designed to get little coders introduced to fundamental programming concepts. At it's core Kaxxt is about robots, lazers and pyramids. What nine year old wouldn't want to play that? When kids play this game they get exposed to ideas such as enumerability or thinking recursively, much in the same fashion that <a href=\"http://mitpress.mit.edu/catalog/item/default.asp?ttype=2&amp;tid=4825\">The Little Schemer</a> tries to &quot;present abstract concepts in a humorous and easy-to-grasp fashion&quot;. The Flash game <a href=\"http://armorgames.com/play/2205/light-bot\">Lightbot</a> also comes to mind. In Lightbot, you visually program an animated robot through a series of puzzles. It seems that robots and lazers may be the key to connecting with today's youth.</p>\n<p>Sadly, _why mysteriously stopped contributing to his projects at some point in 2009, leaving many orphaned works behind. His projects have been adopted by various open source community members. <a href=\"http://blog.steveklabnik.com/2010/11/17/the-hardest-decision-i-ve-ever-made.html\">Steve Klabnik</a> recently quit his job to courageously follow his heart and work on Hackety Hack full time. Similarly, <a href=\"http://kitenet.net/~joey/\">Joey Hess</a> then took the liberty of creating an alpha, playtestable version of Kaxxt that uses the pieces from the board game <a href=\"http://boardgamegeek.com/boardgame/225/icehouse\">Icehouse</a>. His port is called '<a href=\"http://kitenet.net/~joey/code/kaxxt/\">Kaxxt on Ice</a>'</p>\n<p>There are also a few notable academic programming education efforts. <a href=\"http://www.greenfoot.org/\">Greenfoot</a> is designed to teach kids Java. There is an entertaining <a href=\"http://blogs.kent.ac.uk/mik/2008/01/20/teaching-my-daughter-to-code/\">series of posts</a> from a programmer who tried teaching his nine year old daughter how to program using Greenfoot. Google has been lightly researching alternative curriculum designs through their <a href=\"http://www.google.com/educators/index.html\">Google for Educators</a> program. As described by Google's director of education relations in the tech talk &quot;<a href=\"http://www.youtube.com/watch?v=Hz4ZgC_A77g\">Initiatives in Education</a>&quot;, Google has been working on &quot;integrating computing curriculum across K-12 core subjects&quot;. They also recently donated 2 million USD to Sal Khan's <a href=\"http://www.khanacademy.org/\">Khan Academy</a>, which offers a staggering amount of free, digestible education lectures.</p>\n<p>MIT's <a href=\"http://scratch.mit.edu/\">Scratch</a> and Carnegie Mellon's Alice seem to each have years of development poured into them, as well as some friendly competitionand actual in-classroom testing. At first glance, both Scratch and Alice appear deep and involved, much like a complicated programming IDE. I would personally like to see more lightweight artistic interpretations, mockups and spikes of the PhD level programming educational research that is happening between Google, MIT, Carnegie Mellon and elsewhere. In the Hackety Hack Manifesto it is explictly stated that&quot;IDEs are a disaster. Newbs should see only one non-scary window free of tree controls and pinned windows and toolbars. As such, we want to stay away from project files and makefiles, the trappings of an IDE.&quot;</p>\n<p>As part of my fellowship with <a href=\"http://codeforamerica.org/\">Code for America</a> in 2011 I will be working with the city of Boston on software to improve public school education (more details <a href=\"http://codeforamerica.org/2010/11/19/2011-projects-overview-video-and-deck/\">here</a>). I am excited at the prospect of getting to meet those working on Scratch at MIT or researchers from the <a href=\"http://www.concord.org/\">Concord Consortium</a>.</p>\n</body>\n</html>\n"}]}